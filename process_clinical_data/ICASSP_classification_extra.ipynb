{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cb1402",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f171fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from multiprocessing import Pool\n",
    "#from imblearn.over_sampling import KMeansSMOTE, SMOTE\n",
    "from sklearn import preprocessing\n",
    "import math, os\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import resampy\n",
    "import io\n",
    "import msoffcrypto\n",
    "import glob\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import random\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, recall_score, confusion_matrix, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import stats as st\n",
    "import seaborn as sns\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cd03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##predefined filter coeffi}cients, as found by Jan Brond\n",
    "A_coeff = np.array(\n",
    "    [1, -4.1637, 7.5712,-7.9805, 5.385, -2.4636, 0.89238, 0.06361, -1.3481, 2.4734, -2.9257, 2.9298, -2.7816, 2.4777,\n",
    "     -1.6847, 0.46483, 0.46565, -0.67312, 0.4162, -0.13832, 0.019852])\n",
    "B_coeff = np.array(\n",
    "    [0.049109, -0.12284, 0.14356, -0.11269, 0.053804, -0.02023, 0.0063778, 0.018513, -0.038154, 0.048727, -0.052577,\n",
    "     0.047847, -0.046015, 0.036283, -0.012977, -0.0046262, 0.012835, -0.0093762, 0.0034485, -0.00080972, -0.00019623])\n",
    "\n",
    "def pptrunc(data, max_value):\n",
    "    '''\n",
    "    Saturate a vector such that no element's absolute value exceeds max_abs_value.\n",
    "    Current name: absolute_saturate().\n",
    "      :param data: a vector of any dimension containing numerical data\n",
    "      :param max_value: a float value of the absolute value to not exceed\n",
    "      :return: the saturated vector\n",
    "    '''\n",
    "    outd = np.where(data > max_value, max_value, data)\n",
    "    return np.where(outd < -max_value, -max_value, outd)\n",
    "\n",
    "def trunc(data, min_value):\n",
    "  \n",
    "    '''\n",
    "    Truncate a vector such that any value lower than min_value is set to 0.\n",
    "    Current name zero_truncate().\n",
    "    :param data: a vector of any dimension containing numerical data\n",
    "    :param min_value: a float value the elements of data should not fall below\n",
    "    :return: the truncated vector\n",
    "    '''\n",
    "\n",
    "    return np.where(data < min_value, 0, data)\n",
    "\n",
    "def runsum(data, length, threshold):\n",
    "    '''\n",
    "    Compute the running sum of values in a vector exceeding some threshold within a range of indices.\n",
    "    Divides the data into len(data)/length chunks and sums the values in excess of the threshold for each chunk.\n",
    "    Current name run_sum().\n",
    "    :param data: a 1D numerical vector to calculate the sum of\n",
    "    :param len: the length of each chunk to compute a sum along, as a positive integer\n",
    "    :param threshold: a numerical value used to find values exceeding some threshold\n",
    "    :return: a vector of length len(data)/length containing the excess value sum for each chunk of data\n",
    "    '''\n",
    "    \n",
    "    N = len(data)\n",
    "    cnt = int(math.ceil(N/length))\n",
    "\n",
    "    rs = np.zeros(cnt)\n",
    "\n",
    "    for n in range(cnt):\n",
    "        for p in range(length*n, length*(n+1)):\n",
    "            if p<N and data[p]>=threshold:\n",
    "                rs[n] = rs[n] + data[p] - threshold\n",
    "\n",
    "    return rs\n",
    "\n",
    "def counts(data, filesf, B=B_coeff, A=A_coeff):\n",
    "    '''\n",
    "    Get activity counts for a set of accelerometer observations.\n",
    "    First resamples the data frequency to 30Hz, then applies a Butterworth filter to the signal, then filters by the\n",
    "    coefficient matrices, saturates and truncates the result, and applies a running sum to get the final counts.\n",
    "    Current name get_actigraph_counts()\n",
    "    :param data: the vertical axis of accelerometer readings, as a vector\n",
    "    :param filesf: the number of observations per second in the file\n",
    "    :param a: coefficient matrix for filtering the signal, as found by Jan Brond\n",
    "    :param b: coefficient matrix for filtering the signal, as found by Jan Brond\n",
    "    :return: a vector containing the final counts\n",
    "    '''\n",
    "    \n",
    "    deadband = 0.068\n",
    "    sf = 30\n",
    "    peakThreshold = 2.13\n",
    "    adcResolution = 0.0164\n",
    "    integN = 10\n",
    "    gain = 0.965\n",
    "\n",
    "    #if filesf>sf:\n",
    "    data = resampy.resample(np.asarray(data), filesf, sf)\n",
    "\n",
    "    B2, A2 = signal.butter(4, np.array([0.01, 7])/(sf/2), btype='bandpass')\n",
    "    dataf = signal.filtfilt(B2, A2, data)\n",
    "\n",
    "    B = B * gain\n",
    "\n",
    "    #NB: no need for a loop here as we only have one axis in array\n",
    "    fx8up = signal.lfilter(B, A, dataf)\n",
    "\n",
    "    fx8 = pptrunc(fx8up[::3], peakThreshold) #downsampling is replaced by slicing with step parameter\n",
    "\n",
    "    return runsum(np.floor(trunc(np.abs(fx8), deadband)/adcResolution), integN, 0)\n",
    "\n",
    "def POI(sample):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of time spent immobile in a window\n",
    "    \"\"\"\n",
    "    def calc_mob_per_min(countx, county, countz):\n",
    "        mob_per_min = []\n",
    "        for i in range(0, len(countx), 60):\n",
    "            countx_1m = np.mean(countx[i:i+60])\n",
    "            county_1m = np.mean(county[i:i+60])\n",
    "            countz_1m = np.mean(countz[i:i+60])\n",
    "            mob_per_min.append(np.mean([countx_1m, county_1m, countz_1m]))\n",
    "        return mob_per_min\n",
    "\n",
    "    def percentagem_of_immobility(mob_per_min):\n",
    "        mob_per_min = np.asarray(mob_per_min)\n",
    "        inactivity_counts = (mob_per_min <= 4).sum() \n",
    "        return inactivity_counts/len(mob_per_min)\n",
    "\n",
    "    # calculate counts per axis\n",
    "    c1_1s = counts(sample[0], 10)\n",
    "    c2_1s = counts(sample[1], 10)\n",
    "    c3_1s = counts(sample[2], 10)\n",
    "    mob_per_min = calc_mob_per_min(c1_1s, c2_1s, c3_1s)\n",
    "    POI = percentagem_of_immobility(mob_per_min)\n",
    "    return POI\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c05459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude(sample):\n",
    "    mag_vector = []\n",
    "    for s in sample:\n",
    "        mag_vector.append(math.sqrt(sum([s[0]**2, s[1]**2, s[2]**2])))\n",
    "    return mag_vector\n",
    "\n",
    "def A(sample):\n",
    "    feat = []\n",
    "    for col in range(0,sample.shape[1]):\n",
    "        average = np.average(sample[:, col])\n",
    "        feat.append(average)\n",
    "\n",
    "    return np.mean(feat)\n",
    "\n",
    "\n",
    "def SD(sample):\n",
    "    feat = []\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        std = np.std(sample[:, col])\n",
    "        feat.append(std)\n",
    "\n",
    "    return np.mean(feat)\n",
    "\n",
    "\n",
    "def AAD(sample):\n",
    "    feat = []\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        data = sample[col,:]\n",
    "        add = np.mean(np.absolute(data - np.mean(data)))\n",
    "        feat.append(add)\n",
    "\n",
    "    return np.mean(feat)\n",
    "\n",
    "\n",
    "def ARA(sample):\n",
    "    #Average Resultant Acceleration[1]:\n",
    "    # Average of the square roots of the sum of the values of each axis squared √(xi^2 + yi^2+ zi^2) over the ED\n",
    "    feat = []\n",
    "    sum_square = 0\n",
    "    sample = np.power(sample, 2)\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        sum_square = sum_square + sample[:, col]\n",
    "\n",
    "    sample = np.sqrt(sum_square)\n",
    "    average = np.average(sample)\n",
    "    feat.append(average)\n",
    "    return np.mean(feat)\n",
    "\n",
    "def COR(sample):\n",
    "    feat = []\n",
    "    for axis_i in range(0, sample.shape[1]):\n",
    "        for axis_j in range(axis_i+1, sample.shape[1]):\n",
    "            cor = np.corrcoef(sample[:, axis_i], sample[:, axis_j])\n",
    "            cor = 0 if np.isnan(cor) else cor[0][1]\n",
    "            feat.append(cor)\n",
    "\n",
    "    return np.mean(feat)\n",
    "\n",
    "\n",
    "def mag_mean(sample):\n",
    "    mag = magnitude(sample)\n",
    "    ft_mean = np.mean(mag)\n",
    "    return ft_mean\n",
    "\n",
    "def mag_std(sample):\n",
    "    mag = magnitude(sample)\n",
    "    ft_std = np.std(mag)\n",
    "    return ft_std\n",
    "\n",
    "\n",
    "def feature_extraction(sample):\n",
    "    \"\"\"\n",
    "    Derive three activity intensity cues: mean and standard deviation of activity intensity,\n",
    "    and duration of immobility during assessment window to summarize the data.\n",
    "    # Average - A,\n",
    "    # Standard Deviation - SD,\n",
    "    # Average Absolute Difference - AAD,\n",
    "    # Average Resultant Acceleration - ARA(1),\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    mag = magnitude(sample)\n",
    "    features = np.mean(mag)\n",
    "    features = np.hstack((features, np.std(mag)))\n",
    "    features = np.hstack((features, A(sample)))\n",
    "    features = np.hstack((features, SD(sample)))\n",
    "    features = np.hstack((features, AAD(sample)))\n",
    "    features = np.hstack((features, ARA(sample)))\n",
    "    features = np.hstack((features, POI(np.transpose(sample, (1,0)))))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc2d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_patient_map():\n",
    "    # create a map between the subject_deiden_id and the patient id\n",
    "    patient_map = {}\n",
    "    patient_enrollment = pd.read_excel('/data/daily_data/patient_id_mapping.xlsx', engine='openpyxl')\n",
    "\n",
    "    for row in patient_enrollment.itertuples():\n",
    "        patient_map[row.patient_id] = row.subject_deiden_id\n",
    "\n",
    "    return patient_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee0fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo_data(X, y, y_target):\n",
    "    passwd = 'pervasiveICU'\n",
    "\n",
    "    filename = '/home/jsenadesouza/DA-healthy2patient/Pervasive_Sensing_Enrollment_Log.xlsx'\n",
    "    decrypted_workbook = io.BytesIO()\n",
    "    with open(filename, 'rb') as file:\n",
    "        office_file = msoffcrypto.OfficeFile(file)\n",
    "        office_file.load_key(password=passwd)\n",
    "        office_file.decrypt(decrypted_workbook)\n",
    "\n",
    "    ADAPT_enrollment = pd.read_excel(decrypted_workbook, engine=\"openpyxl\")\n",
    "\n",
    "    input_dir = '/data/datasets/ICU_Data/EHR_Data/truncated/2020-02-26/'\n",
    "    df_2016 = pd.read_csv(os.path.join(input_dir, 'encounters_0_trimmed.csv'))\n",
    "\n",
    "    df_2021 = []\n",
    "    files_enc = glob.glob('/data/daily_data/*/encounters*.csv',\n",
    "                          recursive=True)\n",
    "    files_peso = glob.glob('/data/daily_data/*/height_weight*.csv',\n",
    "                          recursive=True)\n",
    "\n",
    "    for file in files_enc:\n",
    "        df = pd.read_csv(file)\n",
    "        df_2021.append(df)\n",
    "\n",
    "    df_2021_peso = []\n",
    "    for file in files_peso:\n",
    "        df = pd.read_csv(file)\n",
    "        df_2021_peso.append(df)\n",
    "\n",
    "\n",
    "    df_2021 = pd.concat(df_2021)\n",
    "    df_2021_peso = pd.concat(df_2021_peso)\n",
    "\n",
    "    patients_char = []\n",
    "    patient_map = set_patient_map()\n",
    "    for patient_id in y_target:\n",
    "        if \"P\" in patient_id or \"I\" in patient_id:\n",
    "            row = df_2021[df_2021['patient_deiden_id'] == patient_map[patient_id]]\n",
    "            height = np.mean(df_2021_peso[(df_2021_peso['patient_deiden_id'] == patient_map[patient_id]) & (df_2021_peso['measurement_name'] == 'weight_kgs')]['measurement_value'].values)\n",
    "            weight = df_2021_peso[(df_2021_peso['patient_deiden_id'] == patient_map[patient_id]) & (df_2021_peso['measurement_name'] == 'height_cm')]['measurement_value'].values[0]\n",
    "            if \"I\" in patient_id:\n",
    "                try:\n",
    "                    admit = datetime.strptime(ADAPT_enrollment['ICU_admit'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0], '%m/%d/%y %H%M')\n",
    "                except:\n",
    "                    try:\n",
    "                        admit = datetime.strptime(ADAPT_enrollment['ICU_admit'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0], '%m/%d/%Y %H%M')\n",
    "                    except:\n",
    "                        print(f\"admit: {ADAPT_enrollment['ICU_admit'][ADAPT_enrollment['Record ID'] == patient_id].values}\")\n",
    "                try:\n",
    "                    consent = pd.Timestamp(ADAPT_enrollment['Consent Date'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0])\n",
    "                except:\n",
    "                    print(f\"consent: {ADAPT_enrollment['Consent Date'][ADAPT_enrollment['Record ID'] == patient_id].values}\")\n",
    "                try:\n",
    "                    dischg = datetime.strptime(ADAPT_enrollment['ICU_dischg'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0], '%m/%d/%y %H%M')\n",
    "                except:\n",
    "                    d_time = ADAPT_enrollment['ICU_dischg'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0]\n",
    "                    if type(d_time) == type(datetime):\n",
    "                        dischg = d_time\n",
    "                    else:              \n",
    "                        dischg = datetime.combine(date.today(), datetime.min.time())\n",
    "            else:\n",
    "                ad_rows = row['admit_datetime'][~row['admit_datetime'].isna()].values\n",
    "                dc_rows = row['dischg_datetime'][~row['dischg_datetime'].isna()].values\n",
    "                try:\n",
    "                    admit = datetime.strptime(min(ad_rows), '%Y-%m-%d %H:%M:%S')\n",
    "                except:\n",
    "                    admit = datetime.strptime(min(ad_rows), '%Y-%m-%d')\n",
    "                try:\n",
    "                    dischg = datetime.strptime(max(dc_rows), '%Y-%m-%d %H:%M:%S')\n",
    "                except:\n",
    "                    try:\n",
    "                        dischg = datetime.strptime(max(dc_rows), '%Y-%m-%d')\n",
    "                    except:\n",
    "                        print(patient_id)\n",
    "                        print(dc_rows)\n",
    "                        dischg = datetime.combine(date.today(), datetime.min.time())\n",
    "            consent = admit\n",
    "        else:\n",
    "            row = df_2016[df_2016['record_id'] == int(patient_id)]\n",
    "            height = row['height_cm'][~row['height_cm'].isna()].values[0]\n",
    "            weight = row['weight_kgs'][~row['weight_kgs'].isna()].values[0]\n",
    "            ad_rows = row['admit_datetime'][~row['admit_datetime'].isna()].values\n",
    "            dc_rows = row['dischg_datetime'][~row['dischg_datetime'].isna()].values\n",
    "            try:\n",
    "                admit = datetime.strptime(min(ad_rows), '%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                admit = datetime.strptime(min(ad_rows), '%Y-%m-%d')\n",
    "            try:\n",
    "                dischg = datetime.strptime(max(dc_rows), '%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                try:\n",
    "                    dischg = datetime.strptime(max(dc_rows), '%Y-%m-%d')\n",
    "                except:\n",
    "                    print(patient_id)\n",
    "                    print(dc_rows)\n",
    "                    dischg = datetime.combine(date.today(), datetime.min.time())\n",
    "            consent = admit\n",
    "\n",
    "\n",
    "        birth = datetime.strptime(row['birth_date'][~row['birth_date'].isna()].values[0], '%Y-%m-%d')\n",
    "        age = int((consent - birth).days/365)\n",
    "        lenght_stay = abs((dischg - admit).days)\n",
    "\n",
    "        gender = row['sex'][~row['sex'].isna()].values[0]\n",
    "        race = row['race'][~row['race'].isna()].values[0]\n",
    "        ethnicity = row['ethnicity'][~row['ethnicity'].isna()].values[0]\n",
    "        if len(row['aids'][~row['aids'].isna()]) > 0:\n",
    "            aids = row['aids'][~row['aids'].isna()].values[0]\n",
    "        else:\n",
    "            aids = -1\n",
    "        if len(row['cancer'][~row['cancer'].isna()]) > 0:\n",
    "            cancer = row['cancer'][~row['cancer'].isna()].values[0]\n",
    "        else:\n",
    "            cancer = -1\n",
    "        if len(row['cerebrovascular_disease'][~row['cerebrovascular_disease'].isna()]) > 0:\n",
    "            cerebrovascular_disease = row['cerebrovascular_disease'][~row['cerebrovascular_disease'].isna()].values[0]\n",
    "        else:\n",
    "            cerebrovascular_disease = -1\n",
    "        if len(row['dementia'][~row['dementia'].isna()]) > 0:\n",
    "            dementia = row['dementia'][~row['dementia'].isna()].values[0]\n",
    "        else:\n",
    "            dementia = -1\n",
    "        if len(row['paraplegia_hemiplegia'][~row['paraplegia_hemiplegia'].isna()]) > 0:\n",
    "            paraplegia_hemiplegia = row['paraplegia_hemiplegia'][~row['paraplegia_hemiplegia'].isna()].values[0]\n",
    "        else:\n",
    "            paraplegia_hemiplegia = -1\n",
    "        if len(row['smoking_status'][~row['smoking_status'].isna()]) > 0:\n",
    "            smoking_status = row['smoking_status'][~row['smoking_status'].isna()].values[0]\n",
    "        else:\n",
    "            smoking_status = -1\n",
    "        if len(row['chf'][~row['chf'].isna()]) > 0:\n",
    "            chf = row['chf'][~row['chf'].isna()].values[0]\n",
    "        else:\n",
    "            chf = -1\n",
    "        if len(row['copd'][~row['copd'].isna()]) > 0:\n",
    "            copd = row['copd'][~row['copd'].isna()].values[0]\n",
    "        else:\n",
    "            copd = -1\n",
    "\n",
    "        if len(row['diabetes_w_o_complications'][~row['diabetes_w_o_complications'].isna()]) > 0:\n",
    "            diabetes_w_o_complications = row['diabetes_w_o_complications'][~row['diabetes_w_o_complications'].isna()].values[0]\n",
    "        else:\n",
    "            diabetes_w_o_complications = -1    \n",
    "        if len(row['diabetes_w_complications'][~row['diabetes_w_complications'].isna()]) > 0:\n",
    "            diabetes_w_complications = row['diabetes_w_complications'][~row['diabetes_w_complications'].isna()].values[0]\n",
    "        else:\n",
    "            diabetes_w_complications = -1    \n",
    "        if diabetes_w_o_complications == 1 or diabetes_w_complications == 1:\n",
    "            diabetes = 1\n",
    "        elif diabetes_w_o_complications == -1 and diabetes_w_complications == -1:\n",
    "            diabetes = -1\n",
    "        elif diabetes_w_o_complications == 0 and diabetes_w_complications == 0:\n",
    "            diabetes = 0\n",
    "        if len(row['m_i'][~row['m_i'].isna()]) > 0:\n",
    "            m_i = row['m_i'][~row['m_i'].isna()].values[0]\n",
    "        else:\n",
    "            m_i = -1\n",
    "        if len(row['metastatic_carcinoma'][~row['metastatic_carcinoma'].isna()]) > 0:\n",
    "            metastatic_carcinoma = row['metastatic_carcinoma'][~row['metastatic_carcinoma'].isna()].values[0]\n",
    "        else:\n",
    "            metastatic_carcinoma = -1\n",
    "        if len(row['mild_liver_disease'][~row['mild_liver_disease'].isna()]) > 0:\n",
    "            mild_liver_disease = row['mild_liver_disease'][~row['mild_liver_disease'].isna()].values[0]\n",
    "        else:\n",
    "            mild_liver_disease = -1\n",
    "        if len(row['moderate_severe_liver_disease'][~row['moderate_severe_liver_disease'].isna()]) > 0:\n",
    "            moderate_severe_liver_disease = row['moderate_severe_liver_disease'][~row['moderate_severe_liver_disease'].isna()].values[0]\n",
    "        else:\n",
    "            moderate_severe_liver_disease = -1\n",
    "        if mild_liver_disease == 1 or moderate_severe_liver_disease == 1:\n",
    "            liver_disease = 1\n",
    "        elif mild_liver_disease == -1 and moderate_severe_liver_disease == -1:\n",
    "            liver_disease = -1\n",
    "        elif mild_liver_disease == 0 and moderate_severe_liver_disease == 0:\n",
    "            liver_disease = 0\n",
    "        if len(row['peptic_ulcer_disease'][~row['peptic_ulcer_disease'].isna()]) > 0:\n",
    "            peptic_ulcer_disease = row['peptic_ulcer_disease'][~row['peptic_ulcer_disease'].isna()].values[0]\n",
    "        else:\n",
    "            peptic_ulcer_disease = -1\n",
    "        if len(row['peripheral_vascular_disease'][~row['peripheral_vascular_disease'].isna()]) > 0:\n",
    "            peripheral_vascular_disease = row['peripheral_vascular_disease'][~row['peripheral_vascular_disease'].isna()].values[0]\n",
    "        else:\n",
    "            peripheral_vascular_disease = -1\n",
    "        if len(row['renal_disease'][~row['renal_disease'].isna()]) > 0:\n",
    "            renal_disease = row['renal_disease'][~row['renal_disease'].isna()].values[0]\n",
    "        else:\n",
    "            renal_disease = -1\n",
    "        if len(row['rheumatologic_disease'][~row['rheumatologic_disease'].isna()]) > 0:\n",
    "            rheumatologic_disease = row['rheumatologic_disease'][~row['rheumatologic_disease'].isna()].values[0]\n",
    "        else:\n",
    "            rheumatologic_disease = -1\n",
    "\n",
    "\n",
    "        patients_char.append({'patient_id': patient_id, 'sex': gender, 'race': race, 'height_cm': height, \n",
    "                              'age':age, 'weight_kgs':weight, 'lenght_stay':lenght_stay, \n",
    "                              \"ethnicity\":ethnicity, \"aids\":aids, \"cancer\":cancer, \"cerebrovascular_disease\":cerebrovascular_disease,\n",
    "                              \"dementia\":dementia, \"paraplegia_hemiplegia\":paraplegia_hemiplegia, \"smoking_status\":smoking_status,\n",
    "                              \"chf\":chf, \"copd\":copd, \"diabetes\":diabetes, \"m_i\":m_i, \"metastatic_carcinoma\":metastatic_carcinoma, \n",
    "                              \"liver_disease\":liver_disease, \"peptic_ulcer_disease\":peptic_ulcer_disease, \"renal_disease\":renal_disease, \n",
    "                              \"rheumatologic_disease\":rheumatologic_disease\n",
    "                             })\n",
    "\n",
    "\n",
    "\n",
    "    df_char = pd.DataFrame(data=patients_char)\n",
    "\n",
    "    df_char.loc[df_char.sex == 'MALE', 'sex']= 0\n",
    "    df_char.loc[df_char.sex != 'MALE', 'sex']= 1\n",
    "    df_char.loc[df_char.race == 'BLACK', 'race']= 0\n",
    "    df_char.loc[df_char.race != 'BLACK', 'race']= 1\n",
    "    df_char.loc[df_char.ethnicity == 'HISPANIC', 'ethnicity']= 0\n",
    "    df_char.loc[df_char.ethnicity != 'HISPANIC', 'ethnicity']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Former Smoker', 'smoking_status']= 0\n",
    "    df_char.loc[df_char.smoking_status == 'Smoker', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Smoker, Current Status Unknown', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Current Every Day Smoker', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Current Some Day Smoker', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Light Tobacco Smoker', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Never Smoker', 'smoking_status']= 2\n",
    "    df_char.loc[df_char.smoking_status == 'Never Smoker ', 'smoking_status']= 2\n",
    "    df_char.loc[df_char.smoking_status == 'Status Unknown', 'smoking_status']= 3\n",
    "    df_char.loc[df_char.smoking_status == 'Unknown If Ever Smoked', 'smoking_status']= 3\n",
    "    df_char.loc[df_char.smoking_status == 'Current Status Unknown', 'smoking_status']= 3\n",
    "    df_char.loc[df_char.smoking_status == 'Unknown If Ever Smoked', 'smoking_status']= 3\n",
    "    df_char.loc[df_char.smoking_status == 'Never Assessed', 'smoking_status']= 3\n",
    "    \n",
    "\n",
    "    X_char = []\n",
    "    col_patient = y_col_names.index('patient_id')\n",
    "    for xx, sample in zip(X.squeeze(), y):\n",
    "        try:\n",
    "            char_pat = df_char[df_char[\"patient_id\"] == sample[col_patient]]\n",
    "            char_final = list(char_pat.loc[:, char_pat.columns != \"patient_id\"].values[0])\n",
    "            X_char.append(char_final)\n",
    "        except:\n",
    "            print(sample[col_patient])\n",
    "    X_char = np.array(X_char)\n",
    "    return X_char, df_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4149579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(n_classes, cum_acc, cum_recall, cum_precision, cum_auc, cum_f1, cum_recall_macro, cum_precision_macro,\n",
    "                  cum_f1_macro):\n",
    "    current_acc = np.array(cum_acc)\n",
    "    current_auc = np.array(cum_auc)\n",
    "    current_recall_macro = np.array(cum_recall_macro)\n",
    "    current_prec_macro = np.array(cum_precision_macro)\n",
    "    current_f1_macro = np.array(cum_f1_macro)\n",
    "\n",
    "    ci_mean = st.t.interval(0.95, len(current_acc) - 1, loc=np.mean(current_acc), scale=st.sem(current_acc))\n",
    "    ci_auc = st.t.interval(0.95, len(current_auc) - 1, loc=np.mean(current_auc), scale=st.sem(current_auc))\n",
    "    ci_recall_macro = st.t.interval(0.95, len(current_recall_macro) - 1, loc=np.mean(current_recall_macro),\n",
    "                                    scale=st.sem(current_recall_macro))\n",
    "    ci_prec_macro = st.t.interval(0.95, len(current_prec_macro) - 1, loc=np.mean(current_prec_macro),\n",
    "                                  scale=st.sem(current_prec_macro))\n",
    "    ci_f1_macro = st.t.interval(0.95, len(current_f1_macro) - 1, loc=np.mean(current_f1_macro),\n",
    "                                scale=st.sem(current_f1_macro))\n",
    "\n",
    "    print('accuracy: {:.2f} ± {:.2f}\\n'.format(np.mean(current_acc) * 100,\n",
    "                                                          abs(np.mean(current_acc) - ci_mean[0]) * 100))\n",
    "\n",
    "    print('recall_macro: {:.2f} ± {:.2f}\\n'.format(np.mean(current_recall_macro) * 100,\n",
    "                                                              abs(np.mean(current_recall_macro) - ci_recall_macro[\n",
    "                                                                  0]) * 100))\n",
    "    print('precision_macro: {:.2f} ± {:.2f}\\n'.format(np.mean(current_prec_macro) * 100,\n",
    "                                                                 abs(np.mean(current_prec_macro) - ci_prec_macro[\n",
    "                                                                     0]) * 100))\n",
    "    print('f1-score_macro: {:.2f} ± {:.2f}\\n'.format(np.mean(current_f1_macro) * 100,\n",
    "                                                                abs(np.mean(current_f1_macro) - ci_f1_macro[\n",
    "                                                                    0]) * 100))\n",
    "    print('roc_auc: {:.2f} ± {:.2f}\\n'.format(np.mean(current_auc) * 100,\n",
    "                                                         abs(np.mean(current_auc) - ci_auc[0]) * 100))\n",
    "\n",
    "    for class_ in range(n_classes):\n",
    "        print(f\"Class: {class_}\")\n",
    "\n",
    "        current_f1 = np.array(cum_f1)[:, class_]\n",
    "        current_recall = np.array(cum_recall)[:, class_]\n",
    "        current_prec = np.array(cum_precision)[:, class_]\n",
    "\n",
    "        ci_f1 = st.t.interval(0.95, len(current_f1) - 1, loc=np.mean(current_f1), scale=st.sem(current_f1))\n",
    "        ci_recall = st.t.interval(0.95, len(current_recall) - 1, loc=np.mean(current_recall),\n",
    "                                  scale=st.sem(current_recall))\n",
    "        ci_prec = st.t.interval(0.95, len(current_prec) - 1, loc=np.mean(current_prec), scale=st.sem(current_prec))\n",
    "\n",
    "        print('recall: {:.2f} ± {:.2f}\\n'.format(np.mean(current_recall) * 100,\n",
    "                                                            abs(np.mean(current_recall) - ci_recall[0]) * 100))\n",
    "        print('precision: {:.2f} ± {:.2f}\\n'.format(np.mean(current_prec) * 100,\n",
    "                                                               abs(np.mean(current_prec) - ci_prec[0]) * 100))\n",
    "        print('f1-score: {:.2f} ± {:.2f}\\n'.format(np.mean(current_f1) * 100,\n",
    "                                                         abs(np.mean(current_f1) - ci_f1[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a457bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1-score': f1_score(y_true, y_pred, average=None, labels=[0,1]),\n",
    "        'f1-score_macro': f1_score(y_true, y_pred, average=\"macro\", labels=[0, 1]),\n",
    "        'recall': recall_score(y_true, y_pred, average=None, zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'confusion_matrix_norm_true': confusion_matrix(y_true, y_pred, normalize='true'),\n",
    "        'precision': precision_score(y_true, y_pred, average=None, zero_division=0),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788168bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(X, y, col_target):\n",
    "    if '-1' in np.unique(y[:, col_target]):\n",
    "        idxs = np.argwhere(np.array(y[:, col_target]) != '-1')\n",
    "        X = X[idxs]\n",
    "        y = y[idxs]\n",
    "    return np.squeeze(X), np.squeeze(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5b334b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, y_target, labels2idx, patient_splits, folder_idx, logger=None):\n",
    "    # split samples based on patients k-fold cross validation\n",
    "    test_index, train_index = [], []\n",
    "    for patient in patient_splits[folder_idx]:\n",
    "        test_index.extend(list(np.where(y[:, -1] == patient)[0]))\n",
    "    train_index = np.setdiff1d(np.arange(y.shape[0]), test_index)\n",
    "\n",
    "    train_data, train_labels = X[train_index].squeeze(), y_target[train_index].squeeze()\n",
    "    test_data, test_labels = X[test_index].squeeze(), y_target[test_index].squeeze()\n",
    "\n",
    "    train_labels = np.array([labels2idx[label] for label in train_labels])\n",
    "    test_labels = np.array([labels2idx[label] for label in test_labels])\n",
    "\n",
    "\n",
    "    print(f\"Folder {folder_idx + 1}\")\n",
    "    print(f\"Train data: {get_class_distribution(np.unique(train_labels, return_counts=True))}\")\n",
    "    print(f\"Test data: {get_class_distribution(np.unique(test_labels, return_counts=True))}\")\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9adbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff(target, predicted):  # Youden index\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "    Returns\n",
    "    -------\n",
    "    list type, with optimal cutoff value\n",
    "\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr))\n",
    "    roc = pd.DataFrame({'tf': pd.Series(tpr - (1 - fpr), index=i), 'threshold': pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[:1]]\n",
    "\n",
    "\n",
    "    return list(roc_t['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d3ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_distribution(class_info):\n",
    "    names, quant = class_info\n",
    "    str = \"\"\n",
    "    for name, q in zip(names, quant):\n",
    "        str += f\"{name}: {q/sum(quant)*100:.2f}% \"\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ace847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_patients(y, y_target):\n",
    "    df_pat = pd.DataFrame(np.concatenate([np.expand_dims(y[:, -1], axis=1), np.expand_dims(y_target, axis=1)], axis=1), columns=[\"patients\", \"target\"])\n",
    "\n",
    "    patients_mild, patients_severe = {}, {}\n",
    "    patients_both = []\n",
    "    for pat in np.unique(df_pat.patients):\n",
    "        names, counts = np.unique(df_pat[\"target\"][df_pat[\"patients\"] == pat].values, return_counts=True)\n",
    "        if len(names)>1:\n",
    "            print(f\"{pat}: {names[0]}:{counts[0]}, {names[1]}:{counts[1]}\")\n",
    "            patients_both.append(pat)\n",
    "        else:\n",
    "            print(f\"{pat}: {names[0]}:{counts[0]}\")\n",
    "            if int(names[0]) == 0:\n",
    "                patients_mild[f\"{pat}\"] = counts[0]\n",
    "            else:\n",
    "                patients_severe[f\"{pat}\"] = counts[0]\n",
    "\n",
    "    patients_mild = {k: v for k, v in sorted(patients_mild.items(), key=lambda item: item[1], reverse=True)}\n",
    "    patients_severe = {k: v for k, v in sorted(patients_severe.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    #todo add aqui pra colocar automaticamente pacientes com as 2 classes nas folds em sequencia\n",
    "    \n",
    "    count_m = [45,113,26,0,0]\n",
    "    count_s = [39,92,40,0,0]\n",
    "    fold_m = [[],[],[],[],[]]\n",
    "    fold_s = [[],[],[],[],[]]\n",
    "    \n",
    "    while len(patients_both):\n",
    "        for ind in range(len(5)):\n",
    "            p = patients_both.pop()\n",
    "            fold_m[ind].append(p)\n",
    "            fold_s[ind].append(p)      \n",
    "\n",
    "    for k, v in patients_mild.items():\n",
    "        min_idx = np.argmin(count_m)\n",
    "        print(count_m)\n",
    "        print(min_idx)\n",
    "        count_m[min_idx] += v\n",
    "        fold_m[min_idx].append(k)\n",
    "\n",
    "    for k, v in patients_severe.items():\n",
    "        min_idx = np.argmin(count_s)\n",
    "        print(count_s)\n",
    "        print(min_idx)\n",
    "        count_s[min_idx] += v\n",
    "        fold_s[min_idx].append(k)\n",
    "\n",
    "    for i in range(5):\n",
    "        print(fold_m[i])\n",
    "        print(fold_s[i])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(count_m)\n",
    "    print(count_s)\n",
    "    return np.concat([fold_m, fold_s], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6fc3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read data and balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70ce87c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_input_file = \"/home/jsenadesouza/DA-healthy2patient/results/outcomes/dataset/t900_INTELLIGENT_ADAPT_PAIN_15wd_15drop_painprev.npz\"\n",
    "tmp = np.load(data_input_file, allow_pickle=True)\n",
    "X = tmp[\"X\"]\n",
    "y = tmp['y']\n",
    "y_col_names = list(tmp['y_col_names'])\n",
    "\n",
    "col_idx_target = y_col_names.index(\"pain_score\")\n",
    "col_idx_prevpain = y_col_names.index('pain_score_prev')\n",
    "X, y = clean(X, y, col_idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203c641a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features\n",
      "271.1 seconds passed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting Features\")\n",
    "start = time()\n",
    "with Pool(100) as p:\n",
    "        X_feat = p.map(feature_extraction, X)\n",
    "end = time()\n",
    "print(f\"{end-start:.4} seconds passed.\")\n",
    "\n",
    "X_feat = np.array(X_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4214d3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6081d1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(np\u001b[38;5;241m.\u001b[39marray(y[:, col_idx_target], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m))\n\u001b[1;32m      2\u001b[0m prev_pain \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(np\u001b[38;5;241m.\u001b[39marray(y[:, col_idx_prevpain], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Experimento classificação de extremos\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "y_target = np.round(np.array(y[:, col_idx_target], dtype=float))\n",
    "prev_pain = np.round(np.array(y[:, col_idx_prevpain], dtype=float))\n",
    "\n",
    "#Experimento classificação de extremos\n",
    "print(np.unique(y_target, return_counts=True))\n",
    "yy_t = []\n",
    "new_y = []\n",
    "for xx, yy in zip(y_target, y):\n",
    "    if xx == 2:\n",
    "        yy_t.append(0)\n",
    "        new_y.append(yy)\n",
    "    elif x == 10:\n",
    "        yy_t.append(1)\n",
    "        new_y.append(yy)\n",
    "assert len(yy_t) == len(new_y)\n",
    "prev_pain_t = prev_pain\n",
    "yy_t = np.asarray(yy_t)\n",
    "new_y = np.asarray(new_y)\n",
    "print(np.unique(yy_t, return_counts=True))\n",
    "print(np.unique(prev_pain_t, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0891346",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m folds_pat \u001b[38;5;241m=\u001b[39m split_patients(new_y, yy_t)\n\u001b[1;32m      2\u001b[0m folds_idx \u001b[38;5;241m=\u001b[39m [[],[],[],[],[]]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36msplit_patients\u001b[0;34m(y, y_target)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_patients\u001b[39m(y, y_target):\n\u001b[0;32m----> 2\u001b[0m     df_pat \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mconcatenate([np\u001b[38;5;241m.\u001b[39mexpand_dims(\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39mexpand_dims(y_target, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatients\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m     patients_mild, patients_severe \u001b[38;5;241m=\u001b[39m {}, {}\n\u001b[1;32m      5\u001b[0m     patients_both \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "folds_pat = split_patients(new_y, yy_t)\n",
    "folds_idx = [[],[],[],[],[]]\n",
    "for i in range(5):\n",
    "    for pat in folds_pat[i]:\n",
    "        idxs = np.where(new_y[:, -1] == pat)[0]\n",
    "        if len(idxs) > 0:\n",
    "            folds_idx[i].extend(list(idxs))      \n",
    "            \n",
    "folders = [[[],[]],[[],[]],[[],[]],[[],[]],[[],[]]]\n",
    "for i in range(len(folds_idx)):\n",
    "    for j in range(len(folds_idx)):\n",
    "        if i == j:\n",
    "            folders[i][0].append(folds_idx[j])\n",
    "        else:\n",
    "            folders[i][1].extend(folds_idx[j])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d381ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_demo, df_demo = get_demo_data(X, y, y[:, -1])\n",
    "#np.savez_compressed('/home/jsenadesouza/DA-healthy2patient/results/outcomes/dataset/X_demo.npz', X_demo=X_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c476cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_demo = np.load('/home/jsenadesouza/DA-healthy2patient/results/outcomes/dataset/X_demo.npz', allow_pickle=True)['X_demo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_demo = X_demo.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99ca9282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(657,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739e877-cfca-4303-815a-10b776d6f112",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification with XGboost class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85cc8329-e58e-44af-94d2-413b7b1afbbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 9306 is out of bounds for axis 0 with size 657",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m train_idx \u001b[38;5;241m=\u001b[39m folders[folder_idx][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m test_idx \u001b[38;5;241m=\u001b[39m folders[folder_idx][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m train_data, train_labels, test_data, test_labels \u001b[38;5;241m=\u001b[39m X_data[train_idx], yy_t[train_idx], X_data[test_idx], yy_t[test_idx]\n\u001b[1;32m     23\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter(train_labels)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# estimate scale_pos_weight value\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 9306 is out of bounds for axis 0 with size 657"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "cum_acc, cum_recall, cum_precision, cum_auc, cum_f1 = [], [], [], [], []\n",
    "cum_recall_macro, cum_precision_macro, cum_f1_macro = [], [], []\n",
    "\n",
    "X_data = np.concatenate([X_feat, np.expand_dims(prev_pain_t, axis=1)], axis=1)\n",
    "#X_data = np.concatenate([X_demo[:, :7], np.expand_dims(prev_pain_t, axis=1)], axis=1)\n",
    "#X_data = np.concatenate([np.expand_dims(prev_pain_t, axis=1), np.expand_dims(prev_pain_t, axis=1)], axis=1)\n",
    "#X_data=X_demo\n",
    "#X_data=X_feat\n",
    "num_folders=5\n",
    "n_classes=2\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "labels2idx = {k: idx for idx, k in enumerate(np.unique(y_target))}\n",
    "for folder_idx in range(num_folders):\n",
    "    # writer = SummaryWriter(\n",
    "    #     f'/home/jsenadesouza/DA-healthy2patient/results/outcomes/tensorboard/transformers/exp_name/run_{time()}')\n",
    "\n",
    "    # split the data into train, val and test sets\n",
    "    train_idx = folders[folder_idx][0]\n",
    "    test_idx = folders[folder_idx][1]\n",
    "    train_data, train_labels, test_data, test_labels = X_data[train_idx], yy_t[train_idx], X_data[test_idx], yy_t[test_idx]\n",
    "    counter = Counter(train_labels)\n",
    "    # estimate scale_pos_weight value\n",
    "    estimate = counter[0] / counter[1]\n",
    "    print(f'Estimate: {estimate}. {counter}')\n",
    "    # fit model no training data\n",
    "    #model = XGBClassifier(tree_method=\"gpu_hist\", use_label_encoder=False, eval_metric='logloss',\n",
    "    #                      scale_pos_weight=estimate, random_state=42)\n",
    "    model = XGBClassifier(scale_pos_weight=estimate, random_state=seed)\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    y_pred = model.predict(test_data)\n",
    "    #y_score = model.predict_proba(test_data)\n",
    "    # Fits the explainer\n",
    "    #test_data_df = pd.DataFrame(test_data, columns=[\"Mean Magnitude\", \"Std Magnitude\", \"Average\", \"Stardard Deviation\", \"Average Absolute Difference\", \"Average Resultant Acceleration\", \"POI\", \"Prev Pain\"])\n",
    "    #test_data_df = pd.DataFrame(test_data, columns=[\"Mean Magnitude\", \"Prev Pain\"])\n",
    "\n",
    "    #explainer = shap.Explainer(model.predict, test_data_df)\n",
    "    # Calculates the SHAP values - It takes some time\n",
    "    #shap_values = explainer(test_data)\n",
    "\n",
    "    #threshold = Find_Optimal_Cutoff(test_labels, y_score[:, 1])\n",
    "    #print(f'threshold: {threshold}')\n",
    "    #y_pred_2 = list(map(lambda x: 1 if x > threshold else 0, y_score[:, 1]))\n",
    "    #print(confusion_matrix(test_labels, y_pred_2))\n",
    "\n",
    "    def plot_metrics(ground_truth, predicted):\n",
    "        metrics = get_metrics(ground_truth, predicted)\n",
    "        for k, v in metrics.items():\n",
    "            if \"confusion\" in k:\n",
    "                print('Fold {} {}:\\n{}\\n'.format(folder_idx+1, k.capitalize(), v))\n",
    "            else:\n",
    "                print('Fold {} {}: {}\\n'.format(folder_idx+1, k.capitalize(), v))\n",
    "        return metrics\n",
    "\n",
    "    metrics = plot_metrics(test_labels, y_pred)\n",
    "    #metrics = plot_metrics(test_labels, y_pred_2)\n",
    "\n",
    "    cum_acc.append(metrics['accuracy'])\n",
    "    cum_f1.append(metrics['f1-score'])\n",
    "    cum_recall.append(metrics['recall'])\n",
    "    cum_precision.append(metrics['precision'])\n",
    "    cum_auc.append(metrics['roc_auc'])\n",
    "    cum_f1_macro.append(metrics['f1-score_macro'])\n",
    "    cum_recall_macro.append(metrics['recall_macro'])\n",
    "    cum_precision_macro.append(metrics['precision_macro'])\n",
    "\n",
    "print(\"\\n\\n K-fold cross val results\")\n",
    "print_metrics(n_classes, cum_acc, cum_recall, cum_precision, cum_auc, cum_f1, cum_recall_macro, cum_precision_macro, cum_f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bad777",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.DataFrame(test_data, columns=[\"Mean Magnitude\", \"Std Magnitude\", \"Average\", \"Stardard Deviation\", \"Average Absolute Difference\", \"Average Resultant Acceleration\", \"POI\", \"Prev Pain\"])\n",
    "explainer = shap.Explainer(model.predict, test_data_df)\n",
    "#Calculates the SHAP values - It takes some time\n",
    "shap_values = explainer(test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371bb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efa7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.summary_plot(shap_values, plot_type='violin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
