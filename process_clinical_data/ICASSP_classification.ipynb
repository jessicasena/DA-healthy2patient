{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cb1402",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f171fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from multiprocessing import Pool\n",
    "#from imblearn.over_sampling import KMeansSMOTE, SMOTE\n",
    "from sklearn import preprocessing\n",
    "import math, os\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import resampy\n",
    "import io\n",
    "import msoffcrypto\n",
    "import glob\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import random\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, recall_score, confusion_matrix, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import stats as st\n",
    "import seaborn as sns\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cd03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##predefined filter coeffi}cients, as found by Jan Brond\n",
    "A_coeff = np.array(\n",
    "    [1, -4.1637, 7.5712,-7.9805, 5.385, -2.4636, 0.89238, 0.06361, -1.3481, 2.4734, -2.9257, 2.9298, -2.7816, 2.4777,\n",
    "     -1.6847, 0.46483, 0.46565, -0.67312, 0.4162, -0.13832, 0.019852])\n",
    "B_coeff = np.array(\n",
    "    [0.049109, -0.12284, 0.14356, -0.11269, 0.053804, -0.02023, 0.0063778, 0.018513, -0.038154, 0.048727, -0.052577,\n",
    "     0.047847, -0.046015, 0.036283, -0.012977, -0.0046262, 0.012835, -0.0093762, 0.0034485, -0.00080972, -0.00019623])\n",
    "\n",
    "def pptrunc(data, max_value):\n",
    "    '''\n",
    "    Saturate a vector such that no element's absolute value exceeds max_abs_value.\n",
    "    Current name: absolute_saturate().\n",
    "      :param data: a vector of any dimension containing numerical data\n",
    "      :param max_value: a float value of the absolute value to not exceed\n",
    "      :return: the saturated vector\n",
    "    '''\n",
    "    outd = np.where(data > max_value, max_value, data)\n",
    "    return np.where(outd < -max_value, -max_value, outd)\n",
    "\n",
    "def trunc(data, min_value):\n",
    "  \n",
    "    '''\n",
    "    Truncate a vector such that any value lower than min_value is set to 0.\n",
    "    Current name zero_truncate().\n",
    "    :param data: a vector of any dimension containing numerical data\n",
    "    :param min_value: a float value the elements of data should not fall below\n",
    "    :return: the truncated vector\n",
    "    '''\n",
    "\n",
    "    return np.where(data < min_value, 0, data)\n",
    "\n",
    "def runsum(data, length, threshold):\n",
    "    '''\n",
    "    Compute the running sum of values in a vector exceeding some threshold within a range of indices.\n",
    "    Divides the data into len(data)/length chunks and sums the values in excess of the threshold for each chunk.\n",
    "    Current name run_sum().\n",
    "    :param data: a 1D numerical vector to calculate the sum of\n",
    "    :param len: the length of each chunk to compute a sum along, as a positive integer\n",
    "    :param threshold: a numerical value used to find values exceeding some threshold\n",
    "    :return: a vector of length len(data)/length containing the excess value sum for each chunk of data\n",
    "    '''\n",
    "    \n",
    "    N = len(data)\n",
    "    cnt = int(math.ceil(N/length))\n",
    "\n",
    "    rs = np.zeros(cnt)\n",
    "\n",
    "    for n in range(cnt):\n",
    "        for p in range(length*n, length*(n+1)):\n",
    "            if p<N and data[p]>=threshold:\n",
    "                rs[n] = rs[n] + data[p] - threshold\n",
    "\n",
    "    return rs\n",
    "\n",
    "def counts(data, filesf, B=B_coeff, A=A_coeff):\n",
    "    '''\n",
    "    Get activity counts for a set of accelerometer observations.\n",
    "    First resamples the data frequency to 30Hz, then applies a Butterworth filter to the signal, then filters by the\n",
    "    coefficient matrices, saturates and truncates the result, and applies a running sum to get the final counts.\n",
    "    Current name get_actigraph_counts()\n",
    "    :param data: the vertical axis of accelerometer readings, as a vector\n",
    "    :param filesf: the number of observations per second in the file\n",
    "    :param a: coefficient matrix for filtering the signal, as found by Jan Brond\n",
    "    :param b: coefficient matrix for filtering the signal, as found by Jan Brond\n",
    "    :return: a vector containing the final counts\n",
    "    '''\n",
    "    \n",
    "    deadband = 0.068\n",
    "    sf = 30\n",
    "    peakThreshold = 2.13\n",
    "    adcResolution = 0.0164\n",
    "    integN = 10\n",
    "    gain = 0.965\n",
    "\n",
    "    #if filesf>sf:\n",
    "    data = resampy.resample(np.asarray(data), filesf, sf)\n",
    "\n",
    "    B2, A2 = signal.butter(4, np.array([0.01, 7])/(sf/2), btype='bandpass')\n",
    "    dataf = signal.filtfilt(B2, A2, data)\n",
    "\n",
    "    B = B * gain\n",
    "\n",
    "    #NB: no need for a loop here as we only have one axis in array\n",
    "    fx8up = signal.lfilter(B, A, dataf)\n",
    "\n",
    "    fx8 = pptrunc(fx8up[::3], peakThreshold) #downsampling is replaced by slicing with step parameter\n",
    "\n",
    "    return runsum(np.floor(trunc(np.abs(fx8), deadband)/adcResolution), integN, 0)\n",
    "\n",
    "def POI(sample):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of time spent immobile in a window\n",
    "    \"\"\"\n",
    "    def calc_mob_per_min(countx, county, countz):\n",
    "        mob_per_min = []\n",
    "        for i in range(0, len(countx), 60):\n",
    "            countx_1m = np.mean(countx[i:i+60])\n",
    "            county_1m = np.mean(county[i:i+60])\n",
    "            countz_1m = np.mean(countz[i:i+60])\n",
    "            mob_per_min.append(np.mean([countx_1m, county_1m, countz_1m]))\n",
    "        return mob_per_min\n",
    "\n",
    "    def percentagem_of_immobility(mob_per_min):\n",
    "        mob_per_min = np.asarray(mob_per_min)\n",
    "        inactivity_counts = (mob_per_min <= 4).sum() \n",
    "        return inactivity_counts/len(mob_per_min)\n",
    "\n",
    "    # calculate counts per axis\n",
    "    c1_1s = counts(sample[0], 10)\n",
    "    c2_1s = counts(sample[1], 10)\n",
    "    c3_1s = counts(sample[2], 10)\n",
    "    mob_per_min = calc_mob_per_min(c1_1s, c2_1s, c3_1s)\n",
    "    POI = percentagem_of_immobility(mob_per_min)\n",
    "    return POI\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c05459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude(sample):\n",
    "    mag_vector = []\n",
    "    for s in sample:\n",
    "        mag_vector.append(math.sqrt(sum([s[0]**2, s[1]**2, s[2]**2])))\n",
    "    return mag_vector\n",
    "\n",
    "def A(sample):\n",
    "    feat = []\n",
    "    for col in range(0,sample.shape[1]):\n",
    "        average = np.average(sample[:, col])\n",
    "        feat.append(average)\n",
    "\n",
    "    return np.mean(feat)\n",
    "\n",
    "\n",
    "def SD(sample):\n",
    "    feat = []\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        std = np.std(sample[:, col])\n",
    "        feat.append(std)\n",
    "\n",
    "    return np.mean(feat)\n",
    "\n",
    "\n",
    "def AAD(sample):\n",
    "    feat = []\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        data = sample[col,:]\n",
    "        add = np.mean(np.absolute(data - np.mean(data)))\n",
    "        feat.append(add)\n",
    "\n",
    "    return np.mean(feat)\n",
    "\n",
    "\n",
    "def ARA(sample):\n",
    "    #Average Resultant Acceleration[1]:\n",
    "    # Average of the square roots of the sum of the values of each axis squared √(xi^2 + yi^2+ zi^2) over the ED\n",
    "    feat = []\n",
    "    sum_square = 0\n",
    "    sample = np.power(sample, 2)\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        sum_square = sum_square + sample[:, col]\n",
    "\n",
    "    sample = np.sqrt(sum_square)\n",
    "    average = np.average(sample)\n",
    "    feat.append(average)\n",
    "    return np.mean(feat)\n",
    "\n",
    "def COR(sample):\n",
    "    feat = []\n",
    "    for axis_i in range(0, sample.shape[1]):\n",
    "        for axis_j in range(axis_i+1, sample.shape[1]):\n",
    "            cor = np.corrcoef(sample[:, axis_i], sample[:, axis_j])\n",
    "            cor = 0 if np.isnan(cor) else cor[0][1]\n",
    "            feat.append(cor)\n",
    "\n",
    "    return np.mean(feat)\n",
    "\n",
    "\n",
    "def mag_mean(sample):\n",
    "    mag = magnitude(sample)\n",
    "    ft_mean = np.mean(mag)\n",
    "    return ft_mean\n",
    "\n",
    "def mag_std(sample):\n",
    "    mag = magnitude(sample)\n",
    "    ft_std = np.std(mag)\n",
    "    return ft_std\n",
    "\n",
    "\n",
    "def feature_extraction(sample):\n",
    "    \"\"\"\n",
    "    Derive three activity intensity cues: mean and standard deviation of activity intensity,\n",
    "    and duration of immobility during assessment window to summarize the data.\n",
    "    # Average - A,\n",
    "    # Standard Deviation - SD,\n",
    "    # Average Absolute Difference - AAD,\n",
    "    # Average Resultant Acceleration - ARA(1),\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    mag = magnitude(sample)\n",
    "    features = np.mean(mag)\n",
    "    features = np.hstack((features, np.std(mag)))\n",
    "    features = np.hstack((features, A(sample)))\n",
    "    features = np.hstack((features, SD(sample)))\n",
    "    features = np.hstack((features, AAD(sample)))\n",
    "    features = np.hstack((features, ARA(sample)))\n",
    "    features = np.hstack((features, POI(np.transpose(sample, (1,0)))))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc2d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_patient_map():\n",
    "    # create a map between the subject_deiden_id and the patient id\n",
    "    patient_map = {}\n",
    "    patient_enrollment = pd.read_excel('/data/daily_data/patient_id_mapping.xlsx', engine='openpyxl')\n",
    "\n",
    "    for row in patient_enrollment.itertuples():\n",
    "        patient_map[row.patient_id] = row.subject_deiden_id\n",
    "\n",
    "    return patient_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee0fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo_data(X, y, y_target):\n",
    "    passwd = 'pervasiveICU'\n",
    "\n",
    "    filename = '/home/jsenadesouza/DA-healthy2patient/Pervasive_Sensing_Enrollment_Log.xlsx'\n",
    "    decrypted_workbook = io.BytesIO()\n",
    "    with open(filename, 'rb') as file:\n",
    "        office_file = msoffcrypto.OfficeFile(file)\n",
    "        office_file.load_key(password=passwd)\n",
    "        office_file.decrypt(decrypted_workbook)\n",
    "\n",
    "    ADAPT_enrollment = pd.read_excel(decrypted_workbook, engine=\"openpyxl\")\n",
    "\n",
    "    input_dir = '/data/datasets/ICU_Data/EHR_Data/truncated/2020-02-26/'\n",
    "    df_2016 = pd.read_csv(os.path.join(input_dir, 'encounters_0_trimmed.csv'))\n",
    "\n",
    "    df_2021 = []\n",
    "    files_enc = glob.glob('/data/daily_data/*/encounters*.csv',\n",
    "                          recursive=True)\n",
    "    files_peso = glob.glob('/data/daily_data/*/height_weight*.csv',\n",
    "                          recursive=True)\n",
    "\n",
    "    for file in files_enc:\n",
    "        df = pd.read_csv(file)\n",
    "        df_2021.append(df)\n",
    "\n",
    "    df_2021_peso = []\n",
    "    for file in files_peso:\n",
    "        df = pd.read_csv(file)\n",
    "        df_2021_peso.append(df)\n",
    "\n",
    "\n",
    "    df_2021 = pd.concat(df_2021)\n",
    "    df_2021_peso = pd.concat(df_2021_peso)\n",
    "\n",
    "    patients_char = []\n",
    "    patient_map = set_patient_map()\n",
    "    for patient_id in y_target:\n",
    "        if \"P\" in patient_id or \"I\" in patient_id:\n",
    "            row = df_2021[df_2021['patient_deiden_id'] == patient_map[patient_id]]\n",
    "            height = np.mean(df_2021_peso[(df_2021_peso['patient_deiden_id'] == patient_map[patient_id]) & (df_2021_peso['measurement_name'] == 'weight_kgs')]['measurement_value'].values)\n",
    "            weight = df_2021_peso[(df_2021_peso['patient_deiden_id'] == patient_map[patient_id]) & (df_2021_peso['measurement_name'] == 'height_cm')]['measurement_value'].values[0]\n",
    "            if \"I\" in patient_id:\n",
    "                try:\n",
    "                    admit = datetime.strptime(ADAPT_enrollment['ICU_admit'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0], '%m/%d/%y %H%M')\n",
    "                except:\n",
    "                    try:\n",
    "                        admit = datetime.strptime(ADAPT_enrollment['ICU_admit'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0], '%m/%d/%Y %H%M')\n",
    "                    except:\n",
    "                        print(f\"admit: {ADAPT_enrollment['ICU_admit'][ADAPT_enrollment['Record ID'] == patient_id].values}\")\n",
    "                try:\n",
    "                    consent = pd.Timestamp(ADAPT_enrollment['Consent Date'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0])\n",
    "                except:\n",
    "                    print(f\"consent: {ADAPT_enrollment['Consent Date'][ADAPT_enrollment['Record ID'] == patient_id].values}\")\n",
    "                try:\n",
    "                    dischg = datetime.strptime(ADAPT_enrollment['ICU_dischg'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0], '%m/%d/%y %H%M')\n",
    "                except:\n",
    "                    d_time = ADAPT_enrollment['ICU_dischg'][ADAPT_enrollment[\"Record ID\"] == patient_id].values[0]\n",
    "                    if type(d_time) == type(datetime):\n",
    "                        dischg = d_time\n",
    "                    else:              \n",
    "                        dischg = datetime.combine(date.today(), datetime.min.time())\n",
    "            else:\n",
    "                ad_rows = row['admit_datetime'][~row['admit_datetime'].isna()].values\n",
    "                dc_rows = row['dischg_datetime'][~row['dischg_datetime'].isna()].values\n",
    "                try:\n",
    "                    admit = datetime.strptime(min(ad_rows), '%Y-%m-%d %H:%M:%S')\n",
    "                except:\n",
    "                    admit = datetime.strptime(min(ad_rows), '%Y-%m-%d')\n",
    "                try:\n",
    "                    dischg = datetime.strptime(max(dc_rows), '%Y-%m-%d %H:%M:%S')\n",
    "                except:\n",
    "                    try:\n",
    "                        dischg = datetime.strptime(max(dc_rows), '%Y-%m-%d')\n",
    "                    except:\n",
    "                        print(patient_id)\n",
    "                        print(dc_rows)\n",
    "                        dischg = datetime.combine(date.today(), datetime.min.time())\n",
    "            consent = admit\n",
    "        else:\n",
    "            row = df_2016[df_2016['record_id'] == int(patient_id)]\n",
    "            height = row['height_cm'][~row['height_cm'].isna()].values[0]\n",
    "            weight = row['weight_kgs'][~row['weight_kgs'].isna()].values[0]\n",
    "            ad_rows = row['admit_datetime'][~row['admit_datetime'].isna()].values\n",
    "            dc_rows = row['dischg_datetime'][~row['dischg_datetime'].isna()].values\n",
    "            try:\n",
    "                admit = datetime.strptime(min(ad_rows), '%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                admit = datetime.strptime(min(ad_rows), '%Y-%m-%d')\n",
    "            try:\n",
    "                dischg = datetime.strptime(max(dc_rows), '%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                try:\n",
    "                    dischg = datetime.strptime(max(dc_rows), '%Y-%m-%d')\n",
    "                except:\n",
    "                    print(patient_id)\n",
    "                    print(dc_rows)\n",
    "                    dischg = datetime.combine(date.today(), datetime.min.time())\n",
    "            consent = admit\n",
    "\n",
    "\n",
    "        birth = datetime.strptime(row['birth_date'][~row['birth_date'].isna()].values[0], '%Y-%m-%d')\n",
    "        age = int((consent - birth).days/365)\n",
    "        lenght_stay = abs((dischg - admit).days)\n",
    "\n",
    "        gender = row['sex'][~row['sex'].isna()].values[0]\n",
    "        race = row['race'][~row['race'].isna()].values[0]\n",
    "        ethnicity = row['ethnicity'][~row['ethnicity'].isna()].values[0]\n",
    "        if len(row['aids'][~row['aids'].isna()]) > 0:\n",
    "            aids = row['aids'][~row['aids'].isna()].values[0]\n",
    "        else:\n",
    "            aids = -1\n",
    "        if len(row['cancer'][~row['cancer'].isna()]) > 0:\n",
    "            cancer = row['cancer'][~row['cancer'].isna()].values[0]\n",
    "        else:\n",
    "            cancer = -1\n",
    "        if len(row['cerebrovascular_disease'][~row['cerebrovascular_disease'].isna()]) > 0:\n",
    "            cerebrovascular_disease = row['cerebrovascular_disease'][~row['cerebrovascular_disease'].isna()].values[0]\n",
    "        else:\n",
    "            cerebrovascular_disease = -1\n",
    "        if len(row['dementia'][~row['dementia'].isna()]) > 0:\n",
    "            dementia = row['dementia'][~row['dementia'].isna()].values[0]\n",
    "        else:\n",
    "            dementia = -1\n",
    "        if len(row['paraplegia_hemiplegia'][~row['paraplegia_hemiplegia'].isna()]) > 0:\n",
    "            paraplegia_hemiplegia = row['paraplegia_hemiplegia'][~row['paraplegia_hemiplegia'].isna()].values[0]\n",
    "        else:\n",
    "            paraplegia_hemiplegia = -1\n",
    "        if len(row['smoking_status'][~row['smoking_status'].isna()]) > 0:\n",
    "            smoking_status = row['smoking_status'][~row['smoking_status'].isna()].values[0]\n",
    "        else:\n",
    "            smoking_status = -1\n",
    "        if len(row['chf'][~row['chf'].isna()]) > 0:\n",
    "            chf = row['chf'][~row['chf'].isna()].values[0]\n",
    "        else:\n",
    "            chf = -1\n",
    "        if len(row['copd'][~row['copd'].isna()]) > 0:\n",
    "            copd = row['copd'][~row['copd'].isna()].values[0]\n",
    "        else:\n",
    "            copd = -1\n",
    "\n",
    "        if len(row['diabetes_w_o_complications'][~row['diabetes_w_o_complications'].isna()]) > 0:\n",
    "            diabetes_w_o_complications = row['diabetes_w_o_complications'][~row['diabetes_w_o_complications'].isna()].values[0]\n",
    "        else:\n",
    "            diabetes_w_o_complications = -1    \n",
    "        if len(row['diabetes_w_complications'][~row['diabetes_w_complications'].isna()]) > 0:\n",
    "            diabetes_w_complications = row['diabetes_w_complications'][~row['diabetes_w_complications'].isna()].values[0]\n",
    "        else:\n",
    "            diabetes_w_complications = -1    \n",
    "        if diabetes_w_o_complications == 1 or diabetes_w_complications == 1:\n",
    "            diabetes = 1\n",
    "        elif diabetes_w_o_complications == -1 and diabetes_w_complications == -1:\n",
    "            diabetes = -1\n",
    "        elif diabetes_w_o_complications == 0 and diabetes_w_complications == 0:\n",
    "            diabetes = 0\n",
    "        if len(row['m_i'][~row['m_i'].isna()]) > 0:\n",
    "            m_i = row['m_i'][~row['m_i'].isna()].values[0]\n",
    "        else:\n",
    "            m_i = -1\n",
    "        if len(row['metastatic_carcinoma'][~row['metastatic_carcinoma'].isna()]) > 0:\n",
    "            metastatic_carcinoma = row['metastatic_carcinoma'][~row['metastatic_carcinoma'].isna()].values[0]\n",
    "        else:\n",
    "            metastatic_carcinoma = -1\n",
    "        if len(row['mild_liver_disease'][~row['mild_liver_disease'].isna()]) > 0:\n",
    "            mild_liver_disease = row['mild_liver_disease'][~row['mild_liver_disease'].isna()].values[0]\n",
    "        else:\n",
    "            mild_liver_disease = -1\n",
    "        if len(row['moderate_severe_liver_disease'][~row['moderate_severe_liver_disease'].isna()]) > 0:\n",
    "            moderate_severe_liver_disease = row['moderate_severe_liver_disease'][~row['moderate_severe_liver_disease'].isna()].values[0]\n",
    "        else:\n",
    "            moderate_severe_liver_disease = -1\n",
    "        if mild_liver_disease == 1 or moderate_severe_liver_disease == 1:\n",
    "            liver_disease = 1\n",
    "        elif mild_liver_disease == -1 and moderate_severe_liver_disease == -1:\n",
    "            liver_disease = -1\n",
    "        elif mild_liver_disease == 0 and moderate_severe_liver_disease == 0:\n",
    "            liver_disease = 0\n",
    "        if len(row['peptic_ulcer_disease'][~row['peptic_ulcer_disease'].isna()]) > 0:\n",
    "            peptic_ulcer_disease = row['peptic_ulcer_disease'][~row['peptic_ulcer_disease'].isna()].values[0]\n",
    "        else:\n",
    "            peptic_ulcer_disease = -1\n",
    "        if len(row['peripheral_vascular_disease'][~row['peripheral_vascular_disease'].isna()]) > 0:\n",
    "            peripheral_vascular_disease = row['peripheral_vascular_disease'][~row['peripheral_vascular_disease'].isna()].values[0]\n",
    "        else:\n",
    "            peripheral_vascular_disease = -1\n",
    "        if len(row['renal_disease'][~row['renal_disease'].isna()]) > 0:\n",
    "            renal_disease = row['renal_disease'][~row['renal_disease'].isna()].values[0]\n",
    "        else:\n",
    "            renal_disease = -1\n",
    "        if len(row['rheumatologic_disease'][~row['rheumatologic_disease'].isna()]) > 0:\n",
    "            rheumatologic_disease = row['rheumatologic_disease'][~row['rheumatologic_disease'].isna()].values[0]\n",
    "        else:\n",
    "            rheumatologic_disease = -1\n",
    "\n",
    "\n",
    "        patients_char.append({'patient_id': patient_id, 'sex': gender, 'race': race, 'height_cm': height, \n",
    "                              'age':age, 'weight_kgs':weight, 'lenght_stay':lenght_stay, \n",
    "                              \"ethnicity\":ethnicity, \"aids\":aids, \"cancer\":cancer, \"cerebrovascular_disease\":cerebrovascular_disease,\n",
    "                              \"dementia\":dementia, \"paraplegia_hemiplegia\":paraplegia_hemiplegia, \"smoking_status\":smoking_status,\n",
    "                              \"chf\":chf, \"copd\":copd, \"diabetes\":diabetes, \"m_i\":m_i, \"metastatic_carcinoma\":metastatic_carcinoma, \n",
    "                              \"liver_disease\":liver_disease, \"peptic_ulcer_disease\":peptic_ulcer_disease, \"renal_disease\":renal_disease, \n",
    "                              \"rheumatologic_disease\":rheumatologic_disease\n",
    "                             })\n",
    "\n",
    "\n",
    "\n",
    "    df_char = pd.DataFrame(data=patients_char)\n",
    "\n",
    "    df_char.loc[df_char.sex == 'MALE', 'sex']= 0\n",
    "    df_char.loc[df_char.sex != 'MALE', 'sex']= 1\n",
    "    df_char.loc[df_char.race == 'BLACK', 'race']= 0\n",
    "    df_char.loc[df_char.race != 'BLACK', 'race']= 1\n",
    "    df_char.loc[df_char.ethnicity == 'HISPANIC', 'ethnicity']= 0\n",
    "    df_char.loc[df_char.ethnicity != 'HISPANIC', 'ethnicity']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Former Smoker', 'smoking_status']= 0\n",
    "    df_char.loc[df_char.smoking_status == 'Smoker', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Smoker, Current Status Unknown', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Current Every Day Smoker', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Current Some Day Smoker', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Light Tobacco Smoker', 'smoking_status']= 1\n",
    "    df_char.loc[df_char.smoking_status == 'Never Smoker', 'smoking_status']= 2\n",
    "    df_char.loc[df_char.smoking_status == 'Never Smoker ', 'smoking_status']= 2\n",
    "    df_char.loc[df_char.smoking_status == 'Status Unknown', 'smoking_status']= 3\n",
    "    df_char.loc[df_char.smoking_status == 'Unknown If Ever Smoked', 'smoking_status']= 3\n",
    "    df_char.loc[df_char.smoking_status == 'Current Status Unknown', 'smoking_status']= 3\n",
    "    df_char.loc[df_char.smoking_status == 'Unknown If Ever Smoked', 'smoking_status']= 3\n",
    "    df_char.loc[df_char.smoking_status == 'Never Assessed', 'smoking_status']= 3\n",
    "    \n",
    "\n",
    "    X_char = []\n",
    "    col_patient = y_col_names.index('patient_id')\n",
    "    for xx, sample in zip(X.squeeze(), y):\n",
    "        try:\n",
    "            char_pat = df_char[df_char[\"patient_id\"] == sample[col_patient]]\n",
    "            char_final = list(char_pat.loc[:, char_pat.columns != \"patient_id\"].values[0])\n",
    "            X_char.append(char_final)\n",
    "        except:\n",
    "            print(sample[col_patient])\n",
    "    X_char = np.array(X_char)\n",
    "    return X_char, df_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4149579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(n_classes, cum_acc, cum_recall, cum_precision, cum_auc, cum_f1, cum_recall_macro, cum_precision_macro,\n",
    "                  cum_f1_macro):\n",
    "    current_acc = np.array(cum_acc)\n",
    "    current_auc = np.array(cum_auc)\n",
    "    current_recall_macro = np.array(cum_recall_macro)\n",
    "    current_prec_macro = np.array(cum_precision_macro)\n",
    "    current_f1_macro = np.array(cum_f1_macro)\n",
    "\n",
    "    ci_mean = st.t.interval(0.95, len(current_acc) - 1, loc=np.mean(current_acc), scale=st.sem(current_acc))\n",
    "    ci_auc = st.t.interval(0.95, len(current_auc) - 1, loc=np.mean(current_auc), scale=st.sem(current_auc))\n",
    "    ci_recall_macro = st.t.interval(0.95, len(current_recall_macro) - 1, loc=np.mean(current_recall_macro),\n",
    "                                    scale=st.sem(current_recall_macro))\n",
    "    ci_prec_macro = st.t.interval(0.95, len(current_prec_macro) - 1, loc=np.mean(current_prec_macro),\n",
    "                                  scale=st.sem(current_prec_macro))\n",
    "    ci_f1_macro = st.t.interval(0.95, len(current_f1_macro) - 1, loc=np.mean(current_f1_macro),\n",
    "                                scale=st.sem(current_f1_macro))\n",
    "\n",
    "    print('accuracy: {:.2f} ± {:.2f}\\n'.format(np.mean(current_acc) * 100,\n",
    "                                                          abs(np.mean(current_acc) - ci_mean[0]) * 100))\n",
    "\n",
    "    print('recall_macro: {:.2f} ± {:.2f}\\n'.format(np.mean(current_recall_macro) * 100,\n",
    "                                                              abs(np.mean(current_recall_macro) - ci_recall_macro[\n",
    "                                                                  0]) * 100))\n",
    "    print('precision_macro: {:.2f} ± {:.2f}\\n'.format(np.mean(current_prec_macro) * 100,\n",
    "                                                                 abs(np.mean(current_prec_macro) - ci_prec_macro[\n",
    "                                                                     0]) * 100))\n",
    "    print('f1-score_macro: {:.2f} ± {:.2f}\\n'.format(np.mean(current_f1_macro) * 100,\n",
    "                                                                abs(np.mean(current_f1_macro) - ci_f1_macro[\n",
    "                                                                    0]) * 100))\n",
    "    print('roc_auc: {:.2f} ± {:.2f}\\n'.format(np.mean(current_auc) * 100,\n",
    "                                                         abs(np.mean(current_auc) - ci_auc[0]) * 100))\n",
    "\n",
    "    for class_ in range(n_classes):\n",
    "        print(f\"Class: {class_}\")\n",
    "\n",
    "        current_f1 = np.array(cum_f1)[:, class_]\n",
    "        current_recall = np.array(cum_recall)[:, class_]\n",
    "        current_prec = np.array(cum_precision)[:, class_]\n",
    "\n",
    "        ci_f1 = st.t.interval(0.95, len(current_f1) - 1, loc=np.mean(current_f1), scale=st.sem(current_f1))\n",
    "        ci_recall = st.t.interval(0.95, len(current_recall) - 1, loc=np.mean(current_recall),\n",
    "                                  scale=st.sem(current_recall))\n",
    "        ci_prec = st.t.interval(0.95, len(current_prec) - 1, loc=np.mean(current_prec), scale=st.sem(current_prec))\n",
    "\n",
    "        print('recall: {:.2f} ± {:.2f}\\n'.format(np.mean(current_recall) * 100,\n",
    "                                                            abs(np.mean(current_recall) - ci_recall[0]) * 100))\n",
    "        print('precision: {:.2f} ± {:.2f}\\n'.format(np.mean(current_prec) * 100,\n",
    "                                                               abs(np.mean(current_prec) - ci_prec[0]) * 100))\n",
    "        print('f1-score: {:.2f} ± {:.2f}\\n'.format(np.mean(current_f1) * 100,\n",
    "                                                         abs(np.mean(current_f1) - ci_f1[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a457bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1-score': f1_score(y_true, y_pred, average=None, labels=[0,1]),\n",
    "        'f1-score_macro': f1_score(y_true, y_pred, average=\"macro\", labels=[0, 1]),\n",
    "        'recall': recall_score(y_true, y_pred, average=None, zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'confusion_matrix_norm_true': confusion_matrix(y_true, y_pred, normalize='true'),\n",
    "        'precision': precision_score(y_true, y_pred, average=None, zero_division=0),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788168bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(X, y, col_target):\n",
    "    if '-1' in np.unique(y[:, col_target]):\n",
    "        idxs = np.argwhere(np.array(y[:, col_target]) != '-1')\n",
    "        X = X[idxs]\n",
    "        y = y[idxs]\n",
    "    return np.squeeze(X), np.squeeze(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5b334b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, y_target, labels2idx, patient_splits, folder_idx, logger=None):\n",
    "    # split samples based on patients k-fold cross validation\n",
    "    test_index, train_index = [], []\n",
    "    for patient in patient_splits[folder_idx]:\n",
    "        test_index.extend(list(np.where(y[:, -1] == patient)[0]))\n",
    "    train_index = np.setdiff1d(np.arange(y.shape[0]), test_index)\n",
    "\n",
    "    train_data, train_labels = X[train_index].squeeze(), y_target[train_index].squeeze()\n",
    "    test_data, test_labels = X[test_index].squeeze(), y_target[test_index].squeeze()\n",
    "\n",
    "    train_labels = np.array([labels2idx[label] for label in train_labels])\n",
    "    test_labels = np.array([labels2idx[label] for label in test_labels])\n",
    "\n",
    "\n",
    "    print(f\"Folder {folder_idx + 1}\")\n",
    "    print(f\"Train data: {get_class_distribution(np.unique(train_labels, return_counts=True))}\")\n",
    "    print(f\"Test data: {get_class_distribution(np.unique(test_labels, return_counts=True))}\")\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9adbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff(target, predicted):  # Youden index\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "    Returns\n",
    "    -------\n",
    "    list type, with optimal cutoff value\n",
    "\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr))\n",
    "    roc = pd.DataFrame({'tf': pd.Series(tpr - (1 - fpr), index=i), 'threshold': pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[:1]]\n",
    "\n",
    "\n",
    "    return list(roc_t['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d3ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_distribution(class_info):\n",
    "    names, quant = class_info\n",
    "    str = \"\"\n",
    "    for name, q in zip(names, quant):\n",
    "        str += f\"{name}: {q/sum(quant)*100:.2f}% \"\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_patients():\n",
    "    df_pat = pd.DataFrame(np.concatenate([np.expand_dims(y[:, -1], axis=1), np.expand_dims(y_target, axis=1)], axis=1), columns=[\"patients\", \"target\"])\n",
    "\n",
    "    patients_mild, patients_severe = {}, {}\n",
    "    for pat in np.unique(df_pat.patients):\n",
    "        names, counts = np.unique(df_pat[\"target\"][df_pat[\"patients\"] == pat].values, return_counts=True)\n",
    "        if len(names)>1:\n",
    "            print(f\"{pat}: {names[0]}:{counts[0]}, {names[1]}:{counts[1]}\")\n",
    "        else:\n",
    "            #print(f\"{pat}: {names[0]}:{counts[0]}\")\n",
    "            if names[0] == \"mild\":\n",
    "                patients_mild[f\"{pat}\"] = counts[0]\n",
    "            else:\n",
    "                patients_severe[f\"{pat}\"] = counts[0]\n",
    "\n",
    "    patients_mild = {k: v for k, v in sorted(patients_mild.items(), key=lambda item: item[1])}\n",
    "    patients_severe = {k: v for k, v in sorted(patients_severe.items(), key=lambda item: item[1])}\n",
    "\n",
    "    count_m = [0,0,0,0,0]\n",
    "    count_s = [0,0,0,0,0]\n",
    "    fold_m = [[],[],[],[],[]]\n",
    "    fold_s = [[],[],[],[],[]]\n",
    "\n",
    "    for k, v in patients_mild.items():\n",
    "        min_idx = np.argmin(count_m)\n",
    "        count_m[min_idx] += v\n",
    "        fold_m[min_idx].append(k)\n",
    "\n",
    "    for k, v in patients_severe.items():\n",
    "        min_idx = np.argmin(count_s)\n",
    "        count_s[min_idx] += v\n",
    "        fold_s[min_idx].append(k)\n",
    "\n",
    "    #for i in range(5):\n",
    "    #    print(fold_m[i])\n",
    "    #    print(fold_s[i])\n",
    "    #    print(\"\\n\")\n",
    "\n",
    "    print(count_m)\n",
    "    print(count_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6fc3d",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read data and balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70ce87c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_input_file = \"/home/jsenadesouza/DA-healthy2patient/results/outcomes/dataset/t900_INTELLIGENT_ADAPT_PAIN_15wd_15drop_painprev.npz\"\n",
    "tmp = np.load(data_input_file, allow_pickle=True)\n",
    "X = tmp[\"X\"]\n",
    "y = tmp['y']\n",
    "y_col_names = list(tmp['y_col_names'])\n",
    "\n",
    "target_col_name = \"pain_score_class\"\n",
    "col_idx_target = y_col_names.index(target_col_name)\n",
    "X, y = clean(X, y, col_idx_target)\n",
    "y_target = y[:, col_idx_target]\n",
    "\n",
    "col_idx_prevpain = y_col_names.index('pain_score_prev_class')\n",
    "prev_pain = y[:, col_idx_prevpain]\n",
    "prev_pain = np.array([0 if x == \"mild\" else 1 for x in prev_pain])\n",
    "yy_t = np.array([0 if x == \"mild\" else 1 for x in y_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d381ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40974/2044013924.py:23: DtypeWarning: Columns (15,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/tmp/ipykernel_40974/2044013924.py:23: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/tmp/ipykernel_40974/2044013924.py:23: DtypeWarning: Columns (15,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/tmp/ipykernel_40974/2044013924.py:23: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/tmp/ipykernel_40974/2044013924.py:23: DtypeWarning: Columns (11,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/tmp/ipykernel_40974/2044013924.py:23: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n",
      "P070\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "X_demo, df_demo = get_demo_data(X, y, y[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc8f73",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203c641a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features\n",
      "357.1 seconds passed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting Features\")\n",
    "start = time()\n",
    "with Pool(100) as p:\n",
    "        X_feat = p.map(feature_extraction, X)\n",
    "end = time()\n",
    "print(f\"{end-start:.4} seconds passed.\")\n",
    "\n",
    "X_feat = np.array(X_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81900b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_pat = []\n",
    "folds_pat.append(['P023', 'P013', 'I051A', '49', '48', '100', 'I045A', 'P051', 'I028A', '112', '92', '83', 'P037', '22', '29', '35', '64', '58','P046', 'I001A', 'I034A'])\n",
    "folds_pat.append(['I021A', 'I043A', 'I019A', 'I044A', 'I008A', '51', '106', 'P004', 'P007', '82', '69', 'P054', 'P055', '63', '41', '4', '89', 'I027A', 'P024', '17'])\n",
    "folds_pat.append(['P038','P021', 'P017', 'P067', 'I033A', 'I050A', 'P052', '40', 'P015', '109', '90', 'I049A', '103', '14', '81', '60', '20', 'I047A', 'P006', '32'])\n",
    "folds_pat.append(['I052A', '98', 'P029', 'I006A', 'I037A', 'P057', 'I004A', 'P042', 'I018A', 'P028', '25', 'P003', 'I025A', '39', '52', '28', '18', 'I053A', 'I023A', '8'])\n",
    "folds_pat.append(['P010', 'I026A', '95', 'I042A', 'P063', '88', '66', '50', '93', '87', '65', '44', 'I022A', 'P070', '47', '26', '75', 'P009', '13', '15'])\n",
    "\n",
    "folds_idx = [[],[],[],[],[]]\n",
    "for i in range(5):\n",
    "    for pat in folds_pat[i]:\n",
    "        idxs = np.where(y[:, -1] == pat)[0]\n",
    "        folds_idx[i].extend(list(idxs))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec12aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [[[],[]],[[],[]],[[],[]],[[],[]],[[],[]]]\n",
    "for i in range(len(folds_idx)):\n",
    "    #print(f\"Folder {i}\")\n",
    "    for j in range(len(folds_idx)):\n",
    "        if i == j:\n",
    "            #print(f\"Train:{j}\")\n",
    "            folders[i][0].append(folds_idx[j])\n",
    "        else:\n",
    "            #print(f\"Test:{j}\")\n",
    "            folders[i][1].extend(folds_idx[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2b9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_demo = X_demo.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739e877-cfca-4303-815a-10b776d6f112",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification with XGboost class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85cc8329-e58e-44af-94d2-413b7b1afbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40974/2134678440.py:21: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  train_data, train_labels, test_data, test_labels = X_data[train_idx], yy_t[train_idx], X_data[test_idx], yy_t[test_idx]\n",
      "/tmp/ipykernel_40974/2134678440.py:21: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  train_data, train_labels, test_data, test_labels = X_data[train_idx], yy_t[train_idx], X_data[test_idx], yy_t[test_idx]\n",
      "/tmp/ipykernel_40974/2134678440.py:21: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  train_data, train_labels, test_data, test_labels = X_data[train_idx], yy_t[train_idx], X_data[test_idx], yy_t[test_idx]\n",
      "/tmp/ipykernel_40974/2134678440.py:21: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  train_data, train_labels, test_data, test_labels = X_data[train_idx], yy_t[train_idx], X_data[test_idx], yy_t[test_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate: 7.259154929577464. Counter({0: 2577, 1: 355})\n",
      "Fold 1 Accuracy: 0.7234547518315964\n",
      "\n",
      "Fold 1 F1-score: [0.83421997 0.16666667]\n",
      "\n",
      "Fold 1 F1-score_macro: 0.5004433172501959\n",
      "\n",
      "Fold 1 Recall: [0.8686075  0.13900415]\n",
      "\n",
      "Fold 1 Recall_macro: 0.5038058232396146\n",
      "\n",
      "Fold 1 Confusion_matrix:\n",
      "[[6743 1020]\n",
      " [1660  268]]\n",
      "\n",
      "Fold 1 Confusion_matrix_norm_true:\n",
      "[[0.8686075  0.1313925 ]\n",
      " [0.86099585 0.13900415]]\n",
      "\n",
      "Fold 1 Precision: [0.80245151 0.20807453]\n",
      "\n",
      "Fold 1 Precision_macro: 0.5052630197881118\n",
      "\n",
      "Fold 1 Roc_auc: 0.5038058232396146\n",
      "\n",
      "Estimate: 4.4884910485933505. Counter({0: 1755, 1: 391})\n",
      "Fold 2 Accuracy: 0.9241194998568293\n",
      "\n",
      "Fold 2 F1-score: [0.95348429 0.7942014 ]\n",
      "\n",
      "Fold 2 F1-score_macro: 0.8738428439272392\n",
      "\n",
      "Fold 2 Recall: [0.94909726 0.81078224]\n",
      "\n",
      "Fold 2 Recall_macro: 0.8799397518411212\n",
      "\n",
      "Fold 2 Confusion_matrix:\n",
      "[[8148  437]\n",
      " [ 358 1534]]\n",
      "\n",
      "Fold 2 Confusion_matrix_norm_true:\n",
      "[[0.94909726 0.05090274]\n",
      " [0.18921776 0.81078224]]\n",
      "\n",
      "Fold 2 Precision: [0.95791206 0.77828513]\n",
      "\n",
      "Fold 2 Precision_macro: 0.8680985982616741\n",
      "\n",
      "Fold 2 Roc_auc: 0.8799397518411212\n",
      "\n",
      "Estimate: 4.5075. Counter({0: 1803, 1: 400})\n",
      "Fold 3 Accuracy: 0.7257197696737044\n",
      "\n",
      "Fold 3 F1-score: [0.83128689 0.26717949]\n",
      "\n",
      "Fold 3 F1-score_macro: 0.5492331910513729\n",
      "\n",
      "Fold 3 Recall: [0.8247628  0.27668614]\n",
      "\n",
      "Fold 3 Recall_macro: 0.5507244681876168\n",
      "\n",
      "Fold 3 Confusion_matrix:\n",
      "[[7041 1496]\n",
      " [1362  521]]\n",
      "\n",
      "Fold 3 Confusion_matrix_norm_true:\n",
      "[[0.8247628  0.1752372 ]\n",
      " [0.72331386 0.27668614]]\n",
      "\n",
      "Fold 3 Precision: [0.83791503 0.25830441]\n",
      "\n",
      "Fold 3 Precision_macro: 0.5481097214200538\n",
      "\n",
      "Fold 3 Roc_auc: 0.5507244681876168\n",
      "\n",
      "Estimate: 3.94541910331384. Counter({0: 2024, 1: 513})\n",
      "Fold 4 Accuracy: 0.9121554630180448\n",
      "\n",
      "Fold 4 F1-score: [0.94786395 0.72120831]\n",
      "\n",
      "Fold 4 F1-score_macro: 0.8345361295472218\n",
      "\n",
      "Fold 4 Recall: [0.96849447 0.64745763]\n",
      "\n",
      "Fold 4 Recall_macro: 0.8079760478065563\n",
      "\n",
      "Fold 4 Confusion_matrix:\n",
      "[[8054  262]\n",
      " [ 624 1146]]\n",
      "\n",
      "Fold 4 Confusion_matrix_norm_true:\n",
      "[[0.96849447 0.03150553]\n",
      " [0.35254237 0.64745763]]\n",
      "\n",
      "Fold 4 Precision: [0.92809403 0.81392045]\n",
      "\n",
      "Fold 4 Precision_macro: 0.8710072427140731\n",
      "\n",
      "Fold 4 Roc_auc: 0.8079760478065563\n",
      "\n",
      "Estimate: 3.4951923076923075. Counter({0: 2181, 1: 624})\n",
      "Fold 5 Accuracy: 0.9173966184558974\n",
      "\n",
      "Fold 5 F1-score: [0.95229692 0.69222011]\n",
      "\n",
      "Fold 5 F1-score_macro: 0.8222585187811813\n",
      "\n",
      "Fold 5 Recall: [0.9921559  0.54972875]\n",
      "\n",
      "Fold 5 Recall_macro: 0.770942326859455\n",
      "\n",
      "Fold 5 Confusion_matrix:\n",
      "[[8095   64]\n",
      " [ 747  912]]\n",
      "\n",
      "Fold 5 Confusion_matrix_norm_true:\n",
      "[[0.9921559  0.0078441 ]\n",
      " [0.45027125 0.54972875]]\n",
      "\n",
      "Fold 5 Precision: [0.91551685 0.93442623]\n",
      "\n",
      "Fold 5 Precision_macro: 0.9249715404496424\n",
      "\n",
      "Fold 5 Roc_auc: 0.770942326859455\n",
      "\n",
      "\n",
      "\n",
      " K-fold cross val results\n",
      "accuracy: 84.06 ± 13.16\n",
      "\n",
      "recall_macro: 70.27 ± 20.57\n",
      "\n",
      "precision_macro: 74.35 ± 24.81\n",
      "\n",
      "f1-score_macro: 71.61 ± 21.91\n",
      "\n",
      "roc_auc: 70.27 ± 20.57\n",
      "\n",
      "Class: 0\n",
      "recall: 92.06 ± 8.80\n",
      "\n",
      "precision: 88.84 ± 8.11\n",
      "\n",
      "f1-score: 90.38 ± 8.06\n",
      "\n",
      "Class: 1\n",
      "recall: 48.47 ± 33.99\n",
      "\n",
      "precision: 59.86 ± 42.09\n",
      "\n",
      "f1-score: 52.83 ± 35.87\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40974/2134678440.py:21: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  train_data, train_labels, test_data, test_labels = X_data[train_idx], yy_t[train_idx], X_data[test_idx], yy_t[test_idx]\n"
     ]
    }
   ],
   "source": [
    "cum_acc, cum_recall, cum_precision, cum_auc, cum_f1 = [], [], [], [], []\n",
    "cum_recall_macro, cum_precision_macro, cum_f1_macro = [], [], []\n",
    "\n",
    "X_data = np.concatenate([X_feat, X_demo[:, :7], np.expand_dims(prev_pain, axis=1)], axis=1)\n",
    "#X_data = np.concatenate([X_demo[:, :7], np.expand_dims(prev_pain, axis=1)], axis=1)\n",
    "#X_data = np.concatenate([np.expand_dims(prev_pain, axis=1), np.expand_dims(prev_pain, axis=1)], axis=1)\n",
    "#X_data=X_demo\n",
    "#X_data=X_feat\n",
    "num_folders=5\n",
    "n_classes=2\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "labels2idx = {k: idx for idx, k in enumerate(np.unique(y_target))}\n",
    "for folder_idx in range(num_folders):\n",
    "    # writer = SummaryWriter(\n",
    "    #     f'/home/jsenadesouza/DA-healthy2patient/results/outcomes/tensorboard/transformers/exp_name/run_{time()}')\n",
    "\n",
    "    # split the data into train, val and test sets\n",
    "    train_idx = folders[folder_idx][0]\n",
    "    test_idx = folders[folder_idx][1]\n",
    "    train_data, train_labels, test_data, test_labels = X_data[train_idx], yy_t[train_idx], X_data[test_idx], yy_t[test_idx]\n",
    "    counter = Counter(train_labels)\n",
    "    # estimate scale_pos_weight value\n",
    "    estimate = counter[0] / counter[1]\n",
    "    print(f'Estimate: {estimate}. {counter}')\n",
    "    # fit model no training data\n",
    "    #model = XGBClassifier(tree_method=\"gpu_hist\", use_label_encoder=False, eval_metric='logloss',\n",
    "    #                      scale_pos_weight=estimate, random_state=42)\n",
    "    model = XGBClassifier(scale_pos_weight=estimate, random_state=seed)\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    y_pred = model.predict(test_data)\n",
    "    #y_score = model.predict_proba(test_data)\n",
    "\n",
    "    #threshold = Find_Optimal_Cutoff(test_labels, y_score[:, 1])\n",
    "    #print(f'threshold: {threshold}')\n",
    "    #y_pred_2 = list(map(lambda x: 1 if x > threshold else 0, y_score[:, 1]))\n",
    "    #print(confusion_matrix(test_labels, y_pred_2))\n",
    "\n",
    "    def plot_metrics(ground_truth, predicted):\n",
    "        metrics = get_metrics(ground_truth, predicted)\n",
    "        for k, v in metrics.items():\n",
    "            if \"confusion\" in k:\n",
    "                print('Fold {} {}:\\n{}\\n'.format(folder_idx+1, k.capitalize(), v))\n",
    "            else:\n",
    "                print('Fold {} {}: {}\\n'.format(folder_idx+1, k.capitalize(), v))\n",
    "        return metrics\n",
    "\n",
    "    metrics = plot_metrics(test_labels, y_pred)\n",
    "    #metrics = plot_metrics(test_labels, y_pred_2)\n",
    "\n",
    "    cum_acc.append(metrics['accuracy'])\n",
    "    cum_f1.append(metrics['f1-score'])\n",
    "    cum_recall.append(metrics['recall'])\n",
    "    cum_precision.append(metrics['precision'])\n",
    "    cum_auc.append(metrics['roc_auc'])\n",
    "    cum_f1_macro.append(metrics['f1-score_macro'])\n",
    "    cum_recall_macro.append(metrics['recall_macro'])\n",
    "    cum_precision_macro.append(metrics['precision_macro'])\n",
    "\n",
    "print(\"\\n\\n K-fold cross val results\")\n",
    "print_metrics(n_classes, cum_acc, cum_recall, cum_precision, cum_auc, cum_f1, cum_recall_macro, cum_precision_macro, cum_f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f29776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cc467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}